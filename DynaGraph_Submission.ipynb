{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import init\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "random.seed(1992)\n",
        "torch.manual_seed(1992)\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNI_jF2uFMCX",
        "outputId": "81a2a0fb-c190-4d07-ae30-9d0a64a94f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your relevant numpy dataset files\n",
        "\n",
        "data_train = X_train\n",
        "data_val = X_val\n",
        "data_test = X_test\n",
        "label_train = y_train\n",
        "label_val = y_val\n",
        "label_test = y_test\n",
        "\n",
        "# init [num_variables, seq_length, num_classes]\n",
        "num_nodes = data_val.size(-2)\n",
        "\n",
        "seq_length = data_val.size(-1)\n",
        "\n",
        "num_classes = 6 # Define number of classes\n",
        "\n",
        "\n",
        "# convert data & labels to TensorDataset\n",
        "train_dataset = TensorDataset(torch.tensor(data_train), torch.tensor(label_train))\n",
        "val_dataset = TensorDataset(torch.tensor(data_val), torch.tensor(label_val))\n",
        "test_dataset = TensorDataset(torch.tensor(data_test), torch.tensor(label_test))\n",
        "\n",
        "\n",
        "# data_loader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                            batch_size=8,\n",
        "                                            shuffle=True,\n",
        "                                            pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=476,\n",
        "                                            shuffle=True,\n",
        "                                            pin_memory=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                            batch_size=476,\n",
        "                                            shuffle=False,\n",
        "                                            pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8MPhbUQFbdi",
        "outputId": "b565ad07-1528-4ffc-8ec2-f2a5e531f26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e0afd8af5468>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(data_train), torch.tensor(label_train))\n",
            "<ipython-input-19-e0afd8af5468>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(data_val), torch.tensor(label_val))\n",
            "<ipython-input-19-e0afd8af5468>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_dataset = TensorDataset(torch.tensor(data_test), torch.tensor(label_test))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6seT1-CkH46o"
      },
      "outputs": [],
      "source": [
        "class multi_shallow_embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_nodes, k_neighs, num_graphs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_nodes = num_nodes\n",
        "        self.k = k_neighs\n",
        "        self.num_graphs = num_graphs\n",
        "\n",
        "        self.emb_s = Parameter(Tensor(num_graphs, num_nodes, 1))\n",
        "        self.emb_t = Parameter(Tensor(num_graphs, 1, num_nodes))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.xavier_uniform_(self.emb_s)\n",
        "        init.xavier_uniform_(self.emb_t)\n",
        "\n",
        "\n",
        "    def forward(self, device):\n",
        "\n",
        "        # adj: [G, N, N]\n",
        "        adj = torch.matmul(self.emb_s, self.emb_t).to(device)\n",
        "\n",
        "        # remove self-loop\n",
        "        adj = adj.clone()\n",
        "        idx = torch.arange(self.num_nodes, dtype=torch.long, device=device)\n",
        "        adj[:, idx, idx] = float('-inf')\n",
        "\n",
        "        # top-k-edge adj\n",
        "        adj_flat = adj.reshape(self.num_graphs, -1)\n",
        "        indices = adj_flat.topk(k=self.k)[1].reshape(-1)\n",
        "\n",
        "        idx = torch.tensor([ i//self.k for i in range(indices.size(0)) ], device=device)\n",
        "\n",
        "        adj_flat = torch.zeros_like(adj_flat).clone()\n",
        "        adj_flat[idx, indices] = 1.\n",
        "        adj = adj_flat.reshape_as(adj)\n",
        "\n",
        "        return adj\n",
        "\n",
        "\n",
        "class Group_Linear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.groups = groups\n",
        "\n",
        "        self.group_mlp = nn.Conv2d(in_channels * groups, out_channels * groups, kernel_size=(1, 1), groups=groups, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.group_mlp.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, is_reshape: False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): [B, C, N, F] (if not is_reshape), [B, C, G, N, F//G] (if is_reshape)\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        C = x.size(1)\n",
        "        N = x.size(-2)\n",
        "        G = self.groups\n",
        "\n",
        "        if not is_reshape:\n",
        "            # x: [B, C_in, G, N, F//G]\n",
        "            x = x.reshape(B, C, N, G, -1).transpose(2, 3)\n",
        "        # x: [B, G*C_in, N, F//G]\n",
        "        x = x.transpose(1, 2).reshape(B, G*C, N, -1)\n",
        "\n",
        "        out = self.group_mlp(x)\n",
        "        out = out.reshape(B, G, self.out_channels, N, -1).transpose(1, 2)\n",
        "\n",
        "        # out: [B, C_out, G, N, F//G]\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseGCNConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.lin = Group_Linear(in_channels, out_channels, groups, bias=False)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "        init.zeros_(self.bias)\n",
        "\n",
        "    def norm(self, adj: Tensor, add_loop):\n",
        "        if add_loop:\n",
        "            adj = adj.clone()\n",
        "            idx = torch.arange(adj.size(-1), dtype=torch.long, device=adj.device)\n",
        "            adj[:, idx, idx] += 1\n",
        "\n",
        "        deg_inv_sqrt = adj.sum(-1).clamp(min=1).pow(-0.5)\n",
        "\n",
        "        adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n",
        "\n",
        "        return adj\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, adj: Tensor, add_loop=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): [B, C, N, F]\n",
        "            adj (Tensor): [B, G, N, N]\n",
        "        \"\"\"\n",
        "        adj = self.norm(adj, add_loop).unsqueeze(1)\n",
        "\n",
        "        # x: [B, C, G, N, F//G]\n",
        "        x = self.lin(x, False)\n",
        "\n",
        "        out = torch.matmul(adj, x)\n",
        "\n",
        "        # out: [B, C, N, F]\n",
        "        B, C, _, N, _ = out.size()\n",
        "        out = out.transpose(2, 3).reshape(B, C, N, -1)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out = out.transpose(1, -1) + self.bias\n",
        "            out = out.transpose(1, -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "class DenseGINConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, groups=1, eps=0, train_eps=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Multi-layer model\n",
        "        self.mlp = Group_Linear(in_channels, out_channels, groups, bias=False)\n",
        "\n",
        "        # Encoder part\n",
        "        self.encoder_mean = Group_Linear(in_channels, out_channels, groups, bias=False)\n",
        "        self.encoder_logvar = Group_Linear(in_channels, out_channels, groups, bias=False)\n",
        "\n",
        "        # Decoder part (similar to the original DenseGINConv2d)\n",
        "        self.mlp = Group_Linear(out_channels, in_channels, groups, bias=False)  # Adjust output channels\n",
        "\n",
        "\n",
        "        self.init_eps = eps\n",
        "        if train_eps:\n",
        "            self.eps = Parameter(Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', Tensor([eps]))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.mlp.reset_parameters()\n",
        "        self.eps.data.fill_(self.init_eps)\n",
        "\n",
        "    def norm(self, adj: Tensor, add_loop):\n",
        "        if add_loop:\n",
        "            adj = adj.clone()\n",
        "            idx = torch.arange(adj.size(-1), dtype=torch.long, device=adj.device)\n",
        "            adj[..., idx, idx] += 1\n",
        "\n",
        "        deg_inv_sqrt = adj.sum(-1).clamp(min=1).pow(-0.5)\n",
        "\n",
        "        adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n",
        "\n",
        "        return adj\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        # Add an epsilon to prevent very large values\n",
        "        epsilon = 1e-7\n",
        "        std = torch.exp(0.5 * logvar) + epsilon\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def forward(self, x: Tensor, adj: Tensor, add_loop=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): [B, C, N, F]\n",
        "            adj (Tensor): [G, N, N]\n",
        "        \"\"\"\n",
        "        B, C, N, _ = x.size()\n",
        "        G = adj.size(0)\n",
        "\n",
        "        # adj-norm\n",
        "        adj = self.norm(adj, add_loop=False)\n",
        "\n",
        "        # x: [B, C, G, N, F//G]\n",
        "        x = x.reshape(B, C, N, G, -1).transpose(2, 3)\n",
        "\n",
        "        out = torch.matmul(adj, x)\n",
        "\n",
        "        # DYNAMIC\n",
        "        x_pre = x[:, :, :-1, ...]\n",
        "\n",
        "        # out = x[:, :, 1:, ...] + x_pre\n",
        "        out[:, :, 1:, ...] = out[:, :, 1:, ...] + x_pre\n",
        "        # out = torch.cat( [x[:, :, 0, ...].unsqueeze(2), out], dim=2 )\n",
        "\n",
        "        if add_loop:\n",
        "            out = (1 + self.eps) * x + out\n",
        "\n",
        "        # out: [B, C, G, N, F//G]\n",
        "        out = self.mlp(out, True)\n",
        "\n",
        "        # out: [B, C, N, F]\n",
        "        C = out.size(1)\n",
        "        out2 = out.transpose(2, 3).reshape(B, C, N, -1)\n",
        "\n",
        "        # Variational encoding\n",
        "        mean = self.mlp(x,True)\n",
        "        logvar = self.mlp(x,True)\n",
        "\n",
        "\n",
        "        return out2 , logvar, mean\n",
        "\n",
        "class Dense_TimeDiffPool2d(nn.Module):\n",
        "\n",
        "    def __init__(self, pre_nodes, pooled_nodes, kern_size, padding):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: add Normalization\n",
        "        self.time_conv = nn.Conv2d(pre_nodes, pooled_nodes, (1, kern_size), padding=(0, padding))\n",
        "\n",
        "        self.re_param = Parameter(Tensor(kern_size, 1))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.time_conv.reset_parameters()\n",
        "        init.kaiming_uniform_(self.re_param, nonlinearity='relu')\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, adj: Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): [B, C, N, F]\n",
        "            adj (Tensor): [G, N, N]\n",
        "        \"\"\"\n",
        "        x = x.transpose(1, 2)\n",
        "        out = self.time_conv(x)\n",
        "        out = out.transpose(1, 2)\n",
        "\n",
        "        # s: [ N^(l+1), N^l, 1, K ]\n",
        "        s = torch.matmul(self.time_conv.weight, self.re_param).view(out.size(-2), -1)\n",
        "\n",
        "        # TODO: fully-connect, how to decrease time complexity\n",
        "        out_adj = torch.matmul(torch.matmul(s, adj), s.transpose(0, 1))\n",
        "\n",
        "        return out, out_adj\n",
        "\n",
        "\n",
        "\n",
        "class GNNStack(nn.Module):\n",
        "    \"\"\" The stack layers of GNN.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gnn_model_type, num_layers, groups, pool_ratio, kern_size,\n",
        "                 in_dim, hidden_dim, out_dim,\n",
        "                 seq_len, num_nodes, num_classes, dropout=0.7, activation=nn.ReLU()):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_matrix = nn.Parameter(torch.randn(groups, num_nodes, num_nodes))\n",
        "        nn.init.xavier_uniform_(self.attention_matrix)\n",
        "        #******************************************************************************\n",
        "\n",
        "        # TODO: Sparsity Analysis\n",
        "        k_neighs = self.num_nodes = num_nodes\n",
        "\n",
        "        self.num_graphs = groups\n",
        "\n",
        "        self.num_feats = seq_len\n",
        "        if seq_len % groups:\n",
        "            self.num_feats += ( groups - seq_len % groups )\n",
        "        self.g_constr = multi_shallow_embedding(num_nodes, k_neighs, self.num_graphs)\n",
        "\n",
        "        gnn_model, heads = self.build_gnn_model(gnn_model_type)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        assert num_layers >= 1, 'Error: Number of layers is invalid.'\n",
        "        assert num_layers == len(kern_size), 'Error: Number of kernel_size should equal to number of layers.'\n",
        "        paddings = [ (k - 1) // 2 for k in kern_size ]\n",
        "\n",
        "        self.tconvs = nn.ModuleList(\n",
        "            [nn.Conv2d(1, in_dim, (1, kern_size[0]), padding=(0, paddings[0]))] +\n",
        "            [nn.Conv2d(heads * in_dim, hidden_dim, (1, kern_size[layer+1]), padding=(0, paddings[layer+1])) for layer in range(num_layers - 2)] +\n",
        "            [nn.Conv2d(heads * hidden_dim, out_dim, (1, kern_size[-1]), padding=(0, paddings[-1]))]\n",
        "        )\n",
        "\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.lstm_layers.append(nn.RNN(input_size=24, hidden_size=in_dim * 24, batch_first=True))\n",
        "        for _ in range(1, num_layers - 2):\n",
        "            self.lstm_layers.append(nn.RNN(input_size=in_dim * 24, hidden_size=hidden_dim * 24, batch_first=True))\n",
        "        self.lstm_layers.append(nn.RNN(input_size=hidden_dim * 24, hidden_size=out_dim * 24, batch_first=True))\n",
        "\n",
        "        self.gconvs = nn.ModuleList(\n",
        "            [gnn_model(in_dim, heads * in_dim, groups)] +\n",
        "            [gnn_model(hidden_dim, heads * hidden_dim, groups) for _ in range(num_layers - 2)] +\n",
        "            [gnn_model(out_dim, heads * out_dim, groups)]\n",
        "        )\n",
        "\n",
        "        self.bns = nn.ModuleList(\n",
        "            [nn.BatchNorm2d(heads * in_dim)] +\n",
        "            [nn.BatchNorm2d(heads * hidden_dim) for _ in range(num_layers - 2)] +\n",
        "            [nn.BatchNorm2d(heads * out_dim)]\n",
        "        )\n",
        "\n",
        "        self.left_num_nodes = []\n",
        "        for layer in range(num_layers + 1):\n",
        "            left_node = round( num_nodes * (1 - (pool_ratio*layer)) )\n",
        "            if left_node > 0:\n",
        "                self.left_num_nodes.append(left_node)\n",
        "            else:\n",
        "                self.left_num_nodes.append(1)\n",
        "        self.diffpool = nn.ModuleList(\n",
        "            [Dense_TimeDiffPool2d(self.left_num_nodes[layer], self.left_num_nodes[layer+1], kern_size[layer], paddings[layer]) for layer in range(num_layers - 1)] +\n",
        "            [Dense_TimeDiffPool2d(self.left_num_nodes[-2], self.left_num_nodes[-1], kern_size[-1], paddings[-1])]\n",
        "        )\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.activation = activation\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.linear = nn.Linear(heads * out_dim, num_classes)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lstm, gconv, bn, pool in zip(self.lstm_layers, self.gconvs, self.bns, self.diffpool):\n",
        "            lstm.reset_parameters()\n",
        "            gconv.reset_parameters()\n",
        "            bn.reset_parameters()\n",
        "            pool.reset_parameters()\n",
        "\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "\n",
        "    def build_gnn_model(self, model_type):\n",
        "        if model_type == 'dyGCN2d':\n",
        "            return DenseGCNConv2d, 1\n",
        "        if model_type == 'dyGIN2d':\n",
        "            return DenseGINConv2d, 1\n",
        "\n",
        "\n",
        "    def forward(self, inputs: Tensor):\n",
        "\n",
        "        if inputs.size(-1) % self.num_graphs:\n",
        "            pad_size = (self.num_graphs - inputs.size(-1) % self.num_graphs) / 2\n",
        "            x = F.pad(inputs, (int(pad_size), ceil(pad_size)), mode='constant', value=0.0)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        adj = self.g_constr(x.device)\n",
        "\n",
        "        #*******************************************************************\n",
        "        # Attention layer\n",
        "        attention_scores = F.softmax(self.attention_matrix, dim=-1)\n",
        "        adj = adj * attention_scores\n",
        "        #*******************************************************************\n",
        "\n",
        "        for lstm, tconv, gconv, bn, pool in zip(self.lstm_layers, self.tconvs, self.gconvs, self.bns, self.diffpool):\n",
        "            s=x.shape[1]\n",
        "            if s==1:\n",
        "               x1=x.repeat(1, 128, 1,1)\n",
        "            else:\n",
        "               x1=x.repeat(1, 2, 1,1)\n",
        "            batch_size, channels, seq_len, features = x.size()\n",
        "            x = x.view(batch_size, seq_len, channels * features)\n",
        "            # Apply LSTM layer\n",
        "            x, _ = lstm(x)\n",
        "            x = torch.reshape(x, (batch_size, -1, seq_len, features))\n",
        "            x=x+x1\n",
        "\n",
        "            temp, logvar, mean = gconv(x, adj)\n",
        "\n",
        "            x, adj = pool(temp, adj)\n",
        "\n",
        "            x = self.activation(bn(x))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "\n",
        "        out = self.global_pool(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "\n",
        "\n",
        "        return out,logvar, mean,adj, attention_scores\n",
        "\n",
        "\n",
        "model = GNNStack(gnn_model_type='dyGIN2d', num_layers=1,\n",
        "                     groups=6, pool_ratio=0.2, kern_size=[11],\n",
        "                     in_dim=128, hidden_dim=128, out_dim=128,\n",
        "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
        "\n",
        "\n",
        "torch.cuda.set_device(DEVICE)\n",
        "\n",
        "# collect cache\n",
        "gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "model = model.cuda(DEVICE)\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "criterion = nn.BCEWithLogitsLoss().cuda(DEVICE)\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                              patience=10, verbose=True)\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "def balanced_accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
        "    \"Computes balanced accuracy when `inp` and `targ` are the same size.\"\n",
        "\n",
        "    if sigmoid: inp = inp.sigmoid()\n",
        "    pred = inp>thresh\n",
        "\n",
        "    correct = pred==targ.bool()\n",
        "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
        "    TN = torch.logical_and(correct,  (targ==0).bool()).sum()\n",
        "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
        "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
        "\n",
        "    TPR = TP/(TP+FN)\n",
        "    TNR = TN/(TN+FP)\n",
        "    balanced_accuracy = (TPR+TNR)/2\n",
        "    return balanced_accuracy\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "def Fbeta_multi(inp, targ, beta=1.0, thresh=0.5, sigmoid=True):\n",
        "    \"Computes Fbeta when `inp` and `targ` are the same size.\"\n",
        "\n",
        "    if sigmoid: inp = inp.sigmoid()\n",
        "    pred = inp>thresh\n",
        "\n",
        "    correct = pred==targ.bool()\n",
        "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
        "    TN = torch.logical_and(correct,  (targ==0).bool()).sum()\n",
        "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
        "    FP = torch.logical_and(~correct, (targ==0).bool()).sum()\n",
        "\n",
        "    precision = TP/(TP+FP)\n",
        "    recall = TP/(TP+FN)\n",
        "    beta2 = beta*beta\n",
        "\n",
        "    if precision+recall > 0:\n",
        "        Fbeta = (1+beta2)*precision*recall/(beta2*precision+recall)\n",
        "    else:\n",
        "        Fbeta = 0\n",
        "    return Fbeta\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "def recall_multi(inp, targ, thresh=0.5, sigmoid=True):\n",
        "    \"Computes recall when `inp` and `targ` are the same size.\"\n",
        "\n",
        "    if sigmoid: inp = inp.sigmoid()\n",
        "    pred = inp>thresh\n",
        "\n",
        "    correct = pred==targ.bool()\n",
        "    TP = torch.logical_and(correct,  (targ==1).bool()).sum()\n",
        "    FN = torch.logical_and(~correct, (targ==1).bool()).sum()\n",
        "\n",
        "    recall = TP/(TP+FN)\n",
        "    return recall\n",
        "\n",
        "per=1e-8\n",
        "# %% [code]\n",
        "def regularization_loss(adj_original, adj_learned, lambda_reg):\n",
        "\n",
        "    # Get dimensions\n",
        "    n_orig = adj_original.shape[-1]\n",
        "    n_learn = adj_learned.shape[-1]\n",
        "\n",
        "    # Pad the smaller matrix with zeros\n",
        "    if n_orig < n_learn:\n",
        "        pad_size = (0, n_learn - n_orig, 0, n_learn - n_orig)\n",
        "        adj_original = F.pad(adj_original, pad_size, mode='constant', value=0)\n",
        "    elif n_learn < n_orig:\n",
        "        pad_size = (0, n_orig - n_learn, 0, n_orig - n_learn)\n",
        "        adj_learned = F.pad(adj_learned, pad_size, mode='constant', value=0)\n",
        "\n",
        "    return lambda_reg * torch.sum((adj_original - adj_learned) ** 2)\n",
        "\n",
        "def structural_loss(adj_original, adj_learned, mu):\n",
        "    \"\"\"\n",
        "    Calculate the structural loss as the cosine distance between the original\n",
        "    and learned adjacency matrices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get dimensions and pad the smaller matrix\n",
        "    n_orig = adj_original.shape[-1]\n",
        "    n_learn = adj_learned.shape[-1]\n",
        "\n",
        "    if n_orig < n_learn:\n",
        "        pad_size = (0, n_learn - n_orig, 0, n_learn - n_orig)\n",
        "        adj_original = F.pad(adj_original, pad_size, mode='constant', value=0)\n",
        "    elif n_learn < n_orig:\n",
        "        pad_size = (0, n_orig - n_learn, 0, n_orig - n_learn)\n",
        "        adj_learned = F.pad(adj_learned, pad_size, mode='constant', value=0)\n",
        "\n",
        "    adj_original_flat = adj_original.view(-1)\n",
        "    adj_learned_flat = adj_learned.view(-1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    dot_product = torch.dot(adj_original_flat, adj_learned_flat)\n",
        "    norm_orig = torch.norm(adj_original_flat)\n",
        "    norm_learn = torch.norm(adj_learned_flat)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if norm_orig * norm_learn == 0:\n",
        "        cosine_similarity = 0\n",
        "    else:\n",
        "        cosine_similarity = dot_product / (norm_orig * norm_learn)\n",
        "\n",
        "    # Structural loss based on cosine distance\n",
        "    return mu * (1 - cosine_similarity)\n",
        "\n",
        "def initialize_adj_original(num_graphs, num_nodes):\n",
        "    # Random initialization for adj_original\n",
        "    adj_original = torch.rand(num_graphs, num_nodes, num_nodes, device=DEVICE)\n",
        "    # Set self-loops to zero using .fill_diagonal_() for each graph in the batch\n",
        "    for i in range(num_graphs):\n",
        "        adj_original[i].fill_diagonal_(0)\n",
        "    return adj_original\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, lr_scheduler,adj_original):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc', ':6.2f')\n",
        "    f_1 = AverageMeter('F1', ':6.2f')\n",
        "    sensitivity = AverageMeter('Sens', ':6.2f')\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    adj_previous = adj_original\n",
        "    for count, (data, label) in enumerate(train_loader):\n",
        "\n",
        "        # data in cuda\n",
        "        augmented_data_neg = augment_data(data)\n",
        "\n",
        "        # Set 50 percent of the values to 0\n",
        "        mask = np.random.choice([0, 1], size=data.shape, p=[0.2, 0.8])\n",
        "\n",
        "        # Use the mask to set 50% of the elements to zero\n",
        "        augmented_data_pos = torch.tensor(data * mask)\n",
        "\n",
        "        data, augmented_data_neg = data.to(DEVICE).type(torch.float), augmented_data_neg.to(DEVICE).type(torch.float)\n",
        "        augmented_data_pos=augmented_data_pos.to(DEVICE).type(torch.float)\n",
        "        label = label.to(DEVICE).type(torch.float)\n",
        "\n",
        "        # Forward pass\n",
        "        output, logvar, mean,adj_learned,attn  = model(data)\n",
        "\n",
        "        loss_reg = regularization_loss(adj_original, adj_learned, 0.001)\n",
        "        loss_struct = 0\n",
        "        if adj_previous is not None:\n",
        "            loss_struct = structural_loss(adj_previous, adj_learned, 0.001)\n",
        "        loss = criterion(output, label) # + loss_reg + loss_struct\n",
        "        total_loss = loss\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "\n",
        "        acc1 = balanced_accuracy_multi(output, label)\n",
        "        f1 = Fbeta_multi(output, label)\n",
        "        sens = recall_multi(output, label)\n",
        "        top1.update(acc1, data.size(0))\n",
        "        f_1.update(f1, data.size(0))\n",
        "        sensitivity.update(sens, data.size(0))\n",
        "\n",
        "        # compute gradient and do Adam step\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        adj_previous = adj_learned.detach()  # Update adj_previous for the next iteration\n",
        "\n",
        "    lr_scheduler.step(top1.avg)\n",
        "\n",
        "    return top1.avg, losses.avg, f_1.avg, sensitivity.avg, attn\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "def augment_data(data):\n",
        "    # Shuffling along the time axis (axis=2)\n",
        "    shuffled_data = data.clone()\n",
        "    batch_size, channels, time_length, features = data.shape\n",
        "    for i in range(batch_size):\n",
        "        # Generating a random permutation of indices from 0 to time_length - 1\n",
        "        idx = torch.randperm(time_length)\n",
        "        shuffled_data[i, :, :, :] = data[i, :, idx, :]\n",
        "    return shuffled_data\n",
        "\n",
        "\n",
        "# %% [code]\n",
        "def validate(val_loader, model, criterion, adj_original):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    f_1 = AverageMeter('F1', ':6.2f')\n",
        "    sensitivity = AverageMeter('Sens', ':6.2f')\n",
        "\n",
        "    # Switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for count, (data, label) in enumerate(val_loader):\n",
        "            data = data.to(DEVICE).type(torch.float)\n",
        "            label = label.to(DEVICE).type(torch.float)\n",
        "\n",
        "            # Compute output\n",
        "            output, logvar, mean, adj_learned, attn = model(data)\n",
        "\n",
        "            # Calculate KL divergence for the variational part (if it's part of the loss during evaluation)\n",
        "            kl_divergence = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "            primary_loss = criterion(output, label)\n",
        "\n",
        "            loss_reg = regularization_loss(adj_original, adj_learned, 0.1)\n",
        "            loss_struct = structural_loss(adj_original, adj_learned, 0.01)\n",
        "\n",
        "            # Here, we use primary_loss directly for validation\n",
        "            # Since regularization and structural losses are typically training stabilizers\n",
        "            loss = primary_loss + loss_reg + kl_divergence + loss_struct  # You can include or exclude KL divergence based on your specific requirements\n",
        "\n",
        "            # Measure accuracy and record loss\n",
        "            acc1 = balanced_accuracy_multi(output, label)\n",
        "            f1 = Fbeta_multi(output, label)\n",
        "            sens = recall_multi(output, label)\n",
        "\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            top1.update(acc1, data.size(0))\n",
        "            f_1.update(f1, data.size(0))\n",
        "            sensitivity.update(sens, data.size(0))\n",
        "\n",
        "            # Clean-up and memory management\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return top1.avg, losses.avg, f_1.avg, sensitivity.avg,attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train = []\n",
        "acc_train = []\n",
        "loss_val = []\n",
        "acc_val = []\n",
        "epoches = []\n",
        "f1_train = []\n",
        "f1_val = []\n",
        "sens_train = []\n",
        "sens_val = []\n",
        "\n",
        "# init acc\n",
        "best_acc1 = 0\n",
        "best_f1 = 0\n",
        "best_sens = 0\n",
        "adj_original = initialize_adj_original(6, num_nodes=num_nodes)\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoches += [epoch]\n",
        "\n",
        "    # train for one epoch\n",
        "    acc_train_per, loss_train_per, f1_train_per, sens_train_per, temp = train(train_loader, model, criterion, optimizer, lr_scheduler,adj_original)\n",
        "\n",
        "    acc_train += [acc_train_per]\n",
        "    loss_train += [loss_train_per]\n",
        "    f1_train += [f1_train_per]\n",
        "    sens_train += [sens_train_per]\n",
        "\n",
        "    msg = f'TRAIN, epoch {epoch}, loss {loss_train_per}, acc {acc_train_per}'\n",
        "\n",
        "    # evaluate on validation set\n",
        "    acc_val_per, loss_val_per, f1_val_per, sens_val_per, attention = validate(val_loader, model, criterion, adj_original)\n",
        "\n",
        "    acc_val += [acc_val_per]\n",
        "    loss_val += [loss_val_per]\n",
        "    f1_val += [f1_val_per]\n",
        "    sens_val += [sens_val_per]\n",
        "\n",
        "    print(f'VAL, loss {loss_val_per}, acc {acc_val_per}, f1 {f1_val_per}, sens {sens_val_per}')\n",
        "\n",
        "    if acc_val_per > best_acc1:\n",
        "      torch.save(model, 'models/EHRSHOT_1992.pt')\n",
        "      attention_scores_train = temp\n",
        "\n",
        "    # remember best acc\n",
        "    best_acc1 = max(acc_val_per, best_acc1)\n",
        "    best_f1 = max(f1_val_per, best_f1)\n",
        "    best_sens = max(sens_val_per, best_sens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L-EV3pcJ1yh",
        "outputId": "1ed3a2a5-127b-46d0-ed8c-547bb93a8d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-7bc3a4f31da0>:604: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  augmented_data_pos = torch.tensor(data * mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL, loss 0.19888468086719513, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20049485564231873, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20112231373786926, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20223067700862885, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20105482637882233, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20301832258701324, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2020978331565857, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20419423282146454, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2023783028125763, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20403751730918884, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20352806150913239, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20310641825199127, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2028382122516632, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20276802778244019, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20237518846988678, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20436765253543854, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2037253975868225, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2035953402519226, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20362524688243866, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2040192186832428, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20420457422733307, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2050110250711441, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20481695234775543, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20477958023548126, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20550556480884552, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2049051970243454, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20557279884815216, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2045426219701767, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2046760618686676, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20519456267356873, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20488014817237854, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20467905700206757, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20488181710243225, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20473620295524597, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20475146174430847, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20550939440727234, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20494963228702545, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20498794317245483, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20512668788433075, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20558223128318787, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20509514212608337, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2055841088294983, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2054535448551178, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20563511550426483, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2058001458644867, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20602911710739136, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20559445023536682, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20592211186885834, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2056189477443695, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20565848052501678, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20590992271900177, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2057439237833023, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20582854747772217, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20543792843818665, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20565460622310638, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20566970109939575, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2055533528327942, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20571596920490265, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20536033809185028, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20560799539089203, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20564977824687958, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20564612746238708, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2056959569454193, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20575259625911713, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2057887315750122, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20619846880435944, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20599259436130524, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2057441771030426, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2058859020471573, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20606009662151337, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2057591825723648, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20636680722236633, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2061632126569748, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20640160143375397, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20577284693717957, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2058570832014084, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20573802292346954, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2063741832971573, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2060333788394928, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20603477954864502, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20597869157791138, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20596809685230255, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20624782145023346, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20608392357826233, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20598484575748444, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20623156428337097, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2062821239233017, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20599156618118286, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20591691136360168, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20610782504081726, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20596830546855927, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.2061385065317154, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20643068850040436, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20606142282485962, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20594535768032074, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20601584017276764, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20635287463665009, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20640957355499268, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20612983405590057, acc 0.5, f1 0.0, sens 0.0\n",
            "VAL, loss 0.20645052194595337, acc 0.5, f1 0.0, sens 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGP1UVi7H46y",
        "outputId": "ba6ea839-d321-4cea-ecc9-fe05ff0776ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-8243185c4b2b>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('models/EHRSHOT_1992.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000, device='cuda:0') 0 tensor(0., device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "model = torch.load('models/EHRSHOT_1992.pt')\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for count, (data, label) in enumerate(val_loader):\n",
        "\n",
        "        data = data.to(DEVICE).type(torch.float)\n",
        "        label = label.to(DEVICE).type(torch.float)\n",
        "\n",
        "        # compute output\n",
        "        output, _, _, _, attention_scores_test = model(data)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1 = balanced_accuracy_multi(output, label)\n",
        "        f1 = Fbeta_multi(output, label)\n",
        "        sens = recall_multi(output, label)\n",
        "\n",
        "        print(acc1, f1, sens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74rJPR8nH46z",
        "outputId": "2db2de46-15a4-4ae9-fa74-0cbc9ffd4ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5000, device='cuda:0'), 0.0, tensor(0., device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "best_acc1, best_f1, best_sens"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnAluo8JrhDY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}